services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      CUDA_VISIBLE_DEVICES: "0"
      OLLAMA_NUM_PARALLEL: 1
      OLLAMA_MAX_QUEUE: 8
      OLLAMA_KEEP_ALIVE: 600
      OLLAMA_HOST: 0.0.0.0:11434
    gpus: all
    # evita procesos huérfanos
    init: true
    healthcheck:
      # Usa el binario de ollama en vez de curl
      test: ["CMD", "bash", "-lc", "ollama list >/dev/null 2>&1"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 20s
    restart: unless-stopped

  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_data:/root/.ollama
    command: ["/bin/bash","-lc","ollama pull qwen2.5:32b-instruct-q4_1 ; ollama pull qwen2.5:14b-instruct-q4_1 || true"]
    restart: "no"

volumes:
  ollama_data:
